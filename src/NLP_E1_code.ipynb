{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO3AjPq2grAr"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "!python -m spacy download zh_core_web_sm #download chinese pipeline if need to\n",
        "import re\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======== Task 1: Process the English Dataset ========\n",
        "# Load CSV Dataset\n",
        "file_path = \"/data/English_Dataset.csv\"\n",
        "df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
        "# Display the first few rows to verify the dataset structure\n",
        "print(df.head())\n",
        "\n",
        "# Text Cleaning Using spaCy\n",
        "\n",
        "# Load spaCy English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define the cleaning function\n",
        "\n",
        "def clean_en_text(text):\n",
        "    \"\"\"\n",
        "    Cleans input text by:\n",
        "    - Converting to string to handle potential NaN values\n",
        "    - Removing HTML tags\n",
        "    - Converting text to lowercase\n",
        "    - Tokenizing using spaCy\n",
        "    - Keeping only alphabetic words (removes punctuation and numbers)\n",
        "    \"\"\"\n",
        "    text = str(text)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    doc = nlp(text.lower())\n",
        "    words = [token.text for token in doc if not token.is_punct and not token.like_num]\n",
        "    return words\n",
        "\n",
        "# Apply the cleaning function to the 'Text' column\n",
        "df['cleaned_text'] = df['Text'].apply(clean_en_text)\n",
        "\n",
        "# Compute Word Frequencies and Total Word Count\n",
        "\n",
        "# Flatten the cleaned word lists into a single list\n",
        "all_en_words = [word for words in df['cleaned_text'] for word in words]\n",
        "\n",
        "# Compute the total number of words (including duplicates)\n",
        "total_en_word_count = len(all_en_words)\n",
        "print(f\"Total number of English words: {total_en_word_count}\")\n",
        "\n",
        "\n",
        "# Count occurrences of each unique word\n",
        "en_word_counts = Counter(all_en_words)\n",
        "\n",
        "# Compute Word Length Frequencies (Unaggregated)\n",
        "\n",
        "# Create a dictionary {en_word_length: total_frequency}\n",
        "\n",
        "en_length_freq = {} # Initialize an empty dictionary to store the frequency of word lengths\n",
        "for word, freq in en_word_counts.items():# Iterate over all words and their occurrence frequencies\n",
        "    en_word_length = len(word) # Calculate the length of the word (i.e., the number of letters)\n",
        "    en_length_freq[en_word_length] = en_length_freq.get(en_word_length, 0) + freq # Accumulate the frequency of words with the same length\n",
        "\n",
        "print(\"English Word Length Frequency Dictionary (Individual Lengths):\\n\")\n",
        "for length in sorted(en_length_freq.keys()):\n",
        "    print(f\"Length {length}: {en_length_freq[length]}\")\n",
        "\n",
        "# Plot the frequency distribution of individual word lengths\n",
        "x_values = sorted(en_length_freq.keys())\n",
        "y_values = [en_length_freq[en_word_length] for en_word_length in x_values]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_values, y_values, marker='o', linestyle='-', color='green',label= 'English')\n",
        "for x, y in zip(x_values, y_values):\n",
        "    plt.text(x, y, f'{y}', ha='center', va='bottom', fontsize=10)\n",
        "plt.xlabel(\"English Word Length (number of letters)\")\n",
        "plt.ylabel(\"English Total Frequency\")\n",
        "plt.title(\"Original English Word Length Frequency Distribution\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Aggregate Word Lengths into Groups\n",
        "\n",
        "# Define grouped bins: {1-3, 4-6, 7-9, 10-12, 13-15, 16-18}\n",
        "\n",
        "group_bins = {\"1-3\": 0, \"4-6\": 0, \"7-9\": 0, \"10-12\": 0, \"13-15\": 0, \"16-18\": 0}\n",
        "for length, freq in en_length_freq.items():\n",
        "    if 1 <= length <= 3:\n",
        "        group_bins[\"1-3\"] += freq\n",
        "    elif 4 <= length <= 6:\n",
        "        group_bins[\"4-6\"] += freq\n",
        "    elif 7 <= length <= 9:\n",
        "        group_bins[\"7-9\"] += freq\n",
        "    elif 10 <= length <= 12:\n",
        "        group_bins[\"10-12\"] += freq\n",
        "    elif 13 <= length <= 15:\n",
        "        group_bins[\"13-15\"] += freq\n",
        "    elif 16 <= length <= 18:\n",
        "        group_bins[\"16-18\"] += freq\n",
        "\n",
        "print(\"Aggregated Grouped English Word Length Frequency:\")\n",
        "for group in group_bins:\n",
        "    print(f\"Group {group}: {group_bins[group]}\")\n",
        "\n",
        "# Plot the frequency distribution of grouped english word lengths\n",
        "groups_order = [\"1-3\", \"4-6\", \"7-9\", \"10-12\", \"13-15\", \"16-18\"]\n",
        "group_freqs = [group_bins[group] for group in groups_order]\n",
        "x_positions = range(len(groups_order))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_positions, group_freqs, marker='o', linestyle='-', color='blue', label= 'English')\n",
        "plt.xticks(x_positions, groups_order)\n",
        "for x, y in zip(x_positions, group_freqs):\n",
        "    plt.text(x, y, f'{y}', ha='center', va='bottom', fontsize=10)\n",
        "plt.xlabel(\"English Word Length Group\")\n",
        "plt.ylabel(\"English Total Frequency\")\n",
        "plt.title(\"Aggregated English Word Length Frequency Distribution\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Calculate Spearman's Correlation\n",
        "# Spearman's correlation measures the relationship between word length and frequency\n",
        "rho, p_value = spearmanr(x_values, y_values)\n",
        "\n",
        "print(f\"Spearman's Correlation Coefficient: {rho:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "if rho < -0.8 and p_value <= 0.05:\n",
        "    print(\"✅ Strong negative correlation detected. Zipf’s Law of Abbreviation is supported!\")\n",
        "elif -0.8 <= rho < -0.5 and p_value <= 0.05:\n",
        "    print(\"⚠️ Moderate negative correlation detected. Zipf’s Law of Abbreviation is partially supported.\")\n",
        "elif -0.5 <= rho <= 0.3 and p_value <= 0.05:\n",
        "    print(\"❓ Weak negative correlation detected. There is some evidence, but it is not strong.\")\n",
        "elif -0.3 <= rho <= 0 and p_value <= 0.05:\n",
        "    print(\"❌ Very weak or no correlation. Zipf’s Law of Abbreviation is not supported.\")\n",
        "else:\n",
        "    print(\"❓ No statistically significant correlation (p-value > 0.05). The results are inconclusive.\")"
      ],
      "metadata": {
        "id": "QcEnImRRHCCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======== Task 2: Process the English Dataset ========\n",
        "# Load Chinese language model\n",
        "nlp = spacy.load(\"zh_core_web_sm\")\n",
        "\n",
        "# Read text file\n",
        "with open('CLTS.txt', 'r', encoding='utf-8') as file:\n",
        "    cn_texts = file.readlines()\n",
        "\n",
        "# Text processing pipeline\n",
        "all_words = []\n",
        "for text in cn_texts:\n",
        "    # Clean text\n",
        "    clean_text = re.sub(r'[^\\u4e00-\\u9fa5]', '', text.strip())\n",
        "\n",
        "    # Process with spacy\n",
        "    doc = nlp(clean_text)\n",
        "\n",
        "    # Extract tokens with filtering\n",
        "    words = [token.text for token in doc\n",
        "             if not token.is_punct        # Remove punctuation\n",
        "             and not token.is_space       # Remove whitespace\n",
        "             and token.text.strip()]      # Remove empty strings\n",
        "\n",
        "    all_words.extend(words)\n",
        "    # Calculate length frequency\n",
        "cn_length_freq = Counter([len(word) for word in all_words])\n",
        "\n",
        "# Sort by word length\n",
        "sorted_lengths = sorted(cn_length_freq.items())\n",
        "B = [item[0] for item in sorted_lengths]  # word lengths\n",
        "C = [item[1] for item in sorted_lengths]  # frequencies\n",
        "\n",
        "# Calculate Spearman's correlation\n",
        "rho, p_value = stats.spearmanr(B, C)\n",
        "\n",
        "# Create plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Set x-axis ticks\n",
        "max_length = max(B)\n",
        "plt.xticks(np.arange(0, max_length + 1, 1))\n",
        "\n",
        "# Plot line and points\n",
        "plt.plot(B, C, 'ro--', alpha=0.8, linewidth=1, label='Chinese')\n",
        "\n",
        "# Add labels and title\n",
        "plt.legend()\n",
        "plt.title('Word Length Frequency in Chinese Text')\n",
        "plt.xlabel('Word Length')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Add frequency numbers on points\n",
        "for x, y in zip(B, C):\n",
        "    plt.text(x, y, str(y), ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.grid(True, linestyle='--', alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Print statistics\n",
        "print(\"\\nWord Length Statistics:\")\n",
        "for length, freq in sorted_lengths:\n",
        "    print(f\"Length {length}: {freq}\")\n",
        "\n",
        "# Aggregate Word Lengths into Groups\n",
        "# Define grouped bins\n",
        "group_bins = {\"1-2\": 0, \"3-4\": 0, \"5-6\": 0, \"7-8\": 0, \"9-10\": 0, \"11-14\": 0}\n",
        "for length, freq in cn_length_freq.items():\n",
        "    if 1 <= length <= 2:\n",
        "        group_bins[\"1-2\"] += freq\n",
        "    elif 3 <= length <= 4:\n",
        "        group_bins[\"3-4\"] += freq\n",
        "    elif 5 <= length <= 6:\n",
        "        group_bins[\"5-6\"] += freq\n",
        "    elif 7 <= length <= 8:\n",
        "        group_bins[\"7-8\"] += freq\n",
        "    elif 9 <= length <= 10:\n",
        "        group_bins[\"9-10\"] += freq\n",
        "    elif 11 <= length <= 14:\n",
        "        group_bins[\"11-14\"] += freq\n",
        "\n",
        "print(\"Aggregated Grouped Chinese Word Length Frequency:\")\n",
        "for group in group_bins:\n",
        "    print(f\"Group {group}: {group_bins[group]}\")\n",
        "\n",
        "# Plot the frequency distribution of grouped english word lengths\n",
        "groups_order = [\"1-2\", \"3-4\", \"5-6\", \"7-8\", \"9-10\", \"11-14\"]\n",
        "group_freqs = [group_bins[group] for group in groups_order]\n",
        "x_positions = range(len(groups_order))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_positions, group_freqs, marker='o', linestyle='-', color='blue', label= 'Chinese')\n",
        "plt.xticks(x_positions, groups_order)\n",
        "for x, y in zip(x_positions, group_freqs):\n",
        "    plt.text(x, y, f'{y}', ha='center', va='bottom', fontsize=10)\n",
        "plt.xlabel(\"Chinese Word Length Group\")\n",
        "plt.ylabel(\"Total Frequency\")\n",
        "plt.title(\"Aggregated Grouped Chinese Word Length Frequency Distribution\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(f\"\\nSpearman's correlation: rho = {rho:.3f} (p = {p_value:.4f})\")\n",
        "if p_value < 0.05:\n",
        "    if rho < 0:\n",
        "        print(\"Results support Zipf's Law of Abbreviation (significant negative correlation)\")\n",
        "    else:\n",
        "        print(\"Results do not support Zipf's Law of Abbreviation (significant positive correlation)\")\n",
        "else:\n",
        "    print(\"Results are not statistically significant\")"
      ],
      "metadata": {
        "id": "EYMUTgM1IuZw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}